import os
import argparse
import torch
import soundfile as sf
from pathlib import Path
import glob
import subprocess
import tempfile
import shutil
import time
from concurrent.futures import ThreadPoolExecutor

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

def parse_args():
    parser = argparse.ArgumentParser(description="Qwen2.5-Omni Optimized Video Translation")
    
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--video_path", type=str, help="Path to a single video file")
    input_group.add_argument("--input_folder", type=str, help="Path to folder containing video files")
    
    parser.add_argument("--output_path", type=str, help="Output path")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-Omni-7B", help="Model path")
    parser.add_argument("--source_lang", type=str, default="auto", help="Source language")
    parser.add_argument("--target_lang", type=str, default="en", help="Target language")
    parser.add_argument("--use_audio", action="store_true", help="Use audio")
    parser.add_argument("--save_audio", action="store_true", help="Save audio")
    parser.add_argument("--use_flash_attn", action="store_true", help="Use Flash Attention")
    parser.add_argument("--preserve_structure", action="store_true", help="Preserve directory structure")
    parser.add_argument("--max_depth", type=int, default=10, help="Maximum recursion depth")
    
    # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
    parser.add_argument("--max_duration", type=int, default=10, help="Maximum video duration to process (seconds)")
    parser.add_argument("--skip_validation", action="store_true", help="Skip video file validation for speed")
    parser.add_argument("--batch_model_loading", action="store_true", help="Load model once for batch processing")
    parser.add_argument("--parallel_validation", action="store_true", help="Parallel video validation")
    
    return parser.parse_args()

def get_video_info_batch(video_files, skip_validation=False, parallel=False):
    """
    æ‰¹é‡è·å–è§†é¢‘ä¿¡æ¯ï¼Œå‡å°‘ffprobeè°ƒç”¨æ¬¡æ•°
    
    Args:
        video_files: è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        skip_validation: è·³è¿‡éªŒè¯ä»¥æå‡é€Ÿåº¦
        parallel: å¹¶è¡Œå¤„ç†
        
    Returns:
        Dict of video_path: {duration, valid}
    """
    print(f"ğŸ” Analyzing {len(video_files)} video files...")
    start_time = time.time()
    
    def analyze_single_video(video_path):
        """åˆ†æå•ä¸ªè§†é¢‘æ–‡ä»¶"""
        try:
            # ä¸€æ¬¡ffprobeè°ƒç”¨è·å–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
            cmd = [
                'ffprobe', '-v', 'error', 
                '-select_streams', 'v:0',
                '-show_entries', 'stream=width,height:format=duration',
                '-of', 'csv=p=0', video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                if len(lines) >= 2:
                    # width,height
                    dimensions = lines[0].split(',')
                    # duration
                    duration = float(lines[1]) if lines[1] else 0
                    
                    valid = not skip_validation and len(dimensions) == 2 and duration > 0
                    return {
                        'duration': duration,
                        'valid': valid or skip_validation,
                        'width': int(dimensions[0]) if dimensions[0] else 0,
                        'height': int(dimensions[1]) if dimensions[1] else 0
                    }
        except Exception as e:
            print(f"  âš ï¸ Analysis failed for {os.path.basename(video_path)}: {e}")
        
        return {'duration': 0, 'valid': False, 'width': 0, 'height': 0}
    
    # æ ¹æ®æ˜¯å¦å¹¶è¡Œå¤„ç†é€‰æ‹©æ‰§è¡Œæ–¹å¼
    if parallel and len(video_files) > 5:
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(analyze_single_video, video_files))
    else:
        results = [analyze_single_video(video) for video in video_files]
    
    # æ„å»ºç»“æœå­—å…¸
    video_info = {}
    valid_count = 0
    total_duration = 0
    
    for video_path, info in zip(video_files, results):
        video_info[video_path] = info
        if info['valid']:
            valid_count += 1
            total_duration += info['duration']
    
    elapsed_time = time.time() - start_time
    print(f"ğŸ“Š Analysis completed in {elapsed_time:.1f}s:")
    print(f"  âœ… Valid videos: {valid_count}/{len(video_files)}")
    print(f"  â±ï¸  Total duration: {total_duration/60:.1f} minutes")
    print(f"  ğŸš€ Speed: {len(video_files)/elapsed_time:.1f} files/second")
    
    return video_info

def get_video_files_optimized(folder_path, max_depth=10, skip_validation=False, parallel=False):
    """
    ä¼˜åŒ–çš„é€’å½’è§†é¢‘æ–‡ä»¶è·å–
    """
    folder_path = Path(folder_path)
    if not folder_path.exists():
        raise FileNotFoundError(f"Folder not found: {folder_path}")
    
    # æ”¯æŒçš„è§†é¢‘æ‰©å±•åï¼ˆé¢„ç¼–è¯‘æ¨¡å¼æå‡åŒ¹é…é€Ÿåº¦ï¼‰
    video_extensions = {
        '.mp4', '.MP4', '.avi', '.AVI', '.mov', '.MOV', '.mkv', '.MKV',
        '.wmv', '.WMV', '.flv', '.FLV', '.webm', '.WEBM', '.m4v', '.M4V',
        '.3gp', '.3GP'
    }
    
    print(f"ğŸ” Scanning directory: {folder_path}")
    start_time = time.time()
    
    # ä½¿ç”¨rglobè¿›è¡Œå¿«é€Ÿé€’å½’æœç´¢
    all_files = []
    for file_path in folder_path.rglob("*"):
        if (file_path.is_file() and 
            file_path.suffix in video_extensions and
            len(file_path.parts) - len(folder_path.parts) <= max_depth):
            all_files.append(str(file_path))
    
    scan_time = time.time() - start_time
    print(f"ğŸ“ Found {len(all_files)} potential video files in {scan_time:.1f}s")
    
    if not all_files:
        print("Warning: No video files found")
        return []
    
    # æ‰¹é‡åˆ†æè§†é¢‘ä¿¡æ¯
    video_info = get_video_info_batch(all_files, skip_validation, parallel)
    
    # è¿‡æ»¤æœ‰æ•ˆè§†é¢‘å¹¶ç”Ÿæˆç›¸å¯¹è·¯å¾„
    valid_videos = []
    for video_path, info in video_info.items():
        if info['valid']:
            relative_path = Path(video_path).relative_to(folder_path)
            valid_videos.append((video_path, str(relative_path), info))
    
    # æŒ‰è·¯å¾„æ’åº
    valid_videos.sort(key=lambda x: x[1])
    
    print(f"âœ… Final result: {len(valid_videos)} valid videos")
    return valid_videos

def translate_video_optimized(video_path, model, processor, source_lang="auto", target_lang="en", 
                            use_audio=True, save_audio=False, max_duration=10):
    """
    ä¼˜åŒ–çš„è§†é¢‘ç¿»è¯‘å‡½æ•°
    """
    print(f"ğŸ¬ Translating: {os.path.basename(video_path)}")
    
    # å‡†å¤‡å¯¹è¯
    conversation = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
            ],
        },
        {
            "role": "user",
            "content": [
                {"type": "video", "video": video_path},
                {"type": "text", "text": f"ç¿»è¯‘æä¾›çš„è§†é¢‘ä¸­çš„è¯´è¯å†…å®¹åˆ°ä¸­æ–‡ã€‚åªéœ€è¦è¾“å‡ºç¿»è¯‘å†…å®¹åŸæ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚"}
            ],
        },
    ]

    try:
        # å¤„ç†è¾“å…¥
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_audio)
        inputs = processor(text=text, audio=audios, images=images, videos=videos, 
                          return_tensors="pt", padding=True, use_audio_in_video=use_audio)
        inputs = inputs.to(model.device).to(model.dtype)

        # ç”Ÿæˆç¿»è¯‘
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—èŠ‚çœå†…å­˜
            if save_audio:
                text_ids, audio = model.generate(**inputs, use_audio_in_video=use_audio, max_new_tokens=512)
            else:
                text_ids = model.generate(**inputs, use_audio_in_video=use_audio, return_audio=False, max_new_tokens=512)
        
        # è§£ç 
        translation = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        return translation.strip()
        
    except Exception as e:
        print(f"âŒ Translation failed: {e}")
        return ""

def main():
    args = parse_args()
    
    if args.video_path:
        # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
        if not os.path.exists(args.video_path):
            raise FileNotFoundError(f"Video file not found: {args.video_path}")
        
        if not args.output_path:
            args.output_path = "./evaluation/test_data/qwen_result.txt"
        
        os.makedirs(os.path.dirname(os.path.abspath(args.output_path)), exist_ok=True)
        
        # æ£€æŸ¥è§†é¢‘æ—¶é•¿
        if not args.skip_validation:
            video_info = get_video_info_batch([args.video_path], args.skip_validation, False)
            duration = video_info[args.video_path]['duration']
            
            if duration > args.max_duration:
                print(f"âš ï¸ Video duration ({duration:.1f}s) exceeds maximum ({args.max_duration}s)")
                print("Consider using --max_duration option or video segmentation")
                return
            
            print(f"ğŸ“¹ Video duration: {duration:.1f}s - within limits")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ¤– Loading model from {args.model_path}...")
        model_kwargs = {"torch_dtype": "auto", "device_map": "auto"}
        if args.use_flash_attn:
            model_kwargs["attn_implementation"] = "flash_attention_2"
        
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(args.model_path, **model_kwargs)
        processor = Qwen2_5OmniProcessor.from_pretrained(args.model_path)
        
        if not args.save_audio:
            model.disable_talker()
        
        # ç¿»è¯‘è§†é¢‘
        translation = translate_video_optimized(
            args.video_path, model, processor, 
            args.source_lang, args.target_lang,
            args.use_audio, args.save_audio, args.max_duration
        )
        
        # ä¿å­˜ç»“æœ
        with open(args.output_path, "w", encoding="utf-8") as f:
            f.write(translation)
        
        print(f"âœ… Translation saved to {args.output_path}")
        print(f"ğŸ“ Result: {translation[:100]}...")
        
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print(f"ğŸš€ Starting optimized batch processing: {args.input_folder}")
        
        # ä¼˜åŒ–çš„æ–‡ä»¶æ‰«æ
        video_files_info = get_video_files_optimized(
            args.input_folder, args.max_depth, 
            args.skip_validation, args.parallel_validation
        )
        
        if not video_files_info:
            print("No valid video files found. Exiting.")
            return
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if not args.output_path:
            args.output_path = "./evaluation/test_data/optimized_results"
        os.makedirs(args.output_path, exist_ok=True)
        
        # è¿‡æ»¤è¶…é•¿è§†é¢‘
        valid_videos = []
        skipped_long = 0
        
        for video_file, relative_path, info in video_files_info:
            if info['duration'] <= args.max_duration:
                valid_videos.append((video_file, relative_path, info))
            else:
                skipped_long += 1
                print(f"â­ï¸ Skipping long video ({info['duration']:.1f}s): {relative_path}")
        
        print(f"ğŸ“Š Processing {len(valid_videos)} videos (skipped {skipped_long} long videos)")
        
        # æ‰¹é‡æ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼‰
        model = None
        processor = None
        
        if args.batch_model_loading:
            print(f"ğŸ¤– Loading model once for batch processing...")
            model_kwargs = {"torch_dtype": "auto", "device_map": "auto"}
            if args.use_flash_attn:
                model_kwargs["attn_implementation"] = "flash_attention_2"
            
            model = Qwen2_5OmniForConditionalGeneration.from_pretrained(args.model_path, **model_kwargs)
            processor = Qwen2_5OmniProcessor.from_pretrained(args.model_path)
            
            if not args.save_audio:
                model.disable_talker()
        
        # å¤„ç†ç»Ÿè®¡
        successful_count = 0
        failed_count = 0
        skipped_count = 0
        total_start = time.time()
        
        # å¤„ç†æ¯ä¸ªè§†é¢‘
        for i, (video_file, relative_path, info) in enumerate(valid_videos, 1):
            print(f"\n{'='*60}")
            print(f"Processing {i}/{len(valid_videos)}: {os.path.basename(video_file)}")
            print(f"Duration: {info['duration']:.1f}s | Size: {info['width']}x{info['height']}")
            print(f"{'='*60}")
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            if args.preserve_structure:
                output_dir = Path(args.output_path) / Path(relative_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)
                output_path = output_dir / f"{Path(relative_path).stem}.txt"
            else:
                flat_name = str(Path(relative_path).with_suffix('')).replace(os.sep, '_')
                output_path = Path(args.output_path) / f"{flat_name}.txt"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_path):
                print(f"â­ï¸ Skipping: Translation already exists")
                skipped_count += 1
                continue
            
            start_time = time.time()
            
            try:
                # å¦‚æœæ²¡æœ‰é¢„åŠ è½½æ¨¡å‹ï¼Œä¸´æ—¶åŠ è½½
                if not args.batch_model_loading:
                    print(f"ğŸ¤– Loading model...")
                    model_kwargs = {"torch_dtype": "auto", "device_map": "auto"}
                    if args.use_flash_attn:
                        model_kwargs["attn_implementation"] = "flash_attention_2"
                    
                    current_model = Qwen2_5OmniForConditionalGeneration.from_pretrained(args.model_path, **model_kwargs)
                    current_processor = Qwen2_5OmniProcessor.from_pretrained(args.model_path)
                    
                    if not args.save_audio:
                        current_model.disable_talker()
                else:
                    current_model = model
                    current_processor = processor
                
                # ç¿»è¯‘è§†é¢‘
                translation = translate_video_optimized(
                    video_file, current_model, current_processor,
                    args.source_lang, args.target_lang,
                    args.use_audio, args.save_audio, args.max_duration
                )
                
                # ä¿å­˜ç»“æœ
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(translation)
                
                elapsed = time.time() - start_time
                print(f"âœ… Completed in {elapsed:.1f}s: {translation[:50]}...")
                successful_count += 1
                
                # å¦‚æœæ˜¯ä¸´æ—¶åŠ è½½çš„æ¨¡å‹ï¼Œæ¸…ç†å†…å­˜
                if not args.batch_model_loading:
                    del current_model, current_processor
                    torch.cuda.empty_cache()
                
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âŒ Failed after {elapsed:.1f}s: {e}")
                failed_count += 1
        
        # æœ€ç»ˆç»Ÿè®¡
        total_elapsed = time.time() - total_start
        print(f"\n{'='*60}")
        print(f"ğŸ‰ OPTIMIZED BATCH PROCESSING COMPLETED!")
        print(f"{'='*60}")
        print(f"â±ï¸ Total time: {total_elapsed/60:.1f} minutes")
        print(f"ğŸ“Š Processing Summary:")
        print(f"  ğŸ“ Videos found: {len(video_files_info)}")
        print(f"  â­ï¸ Skipped (long): {skipped_long}")
        print(f"  â­ï¸ Skipped (exists): {skipped_count}")
        print(f"  âœ… Successfully processed: {successful_count}")
        print(f"  âŒ Failed: {failed_count}")
        print(f"  ğŸš€ Average speed: {successful_count*60/total_elapsed:.1f} videos/hour")
        print(f"{'='*60}")

if __name__ == "__main__":
    main() 