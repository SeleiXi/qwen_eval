#!/usr/bin/env python3
"""
Optimized Parallel Video Translation Script
æ€§èƒ½ä¼˜åŒ–çš„å¹¶è¡Œè§†é¢‘ç¿»è¯‘è„šæœ¬

ä¸»è¦ä¼˜åŒ–ï¼š
1. ä¸­é—´çŠ¶æ€ä¿å­˜å’Œæ–­ç‚¹ç»­ä¼ 
2. æ‰¹å¤„ç†ä¼˜åŒ–å‡å°‘GPUåˆ‡æ¢å¼€é”€  
3. èµ„æºç®¡ç†ä¼˜åŒ–
4. æ›´å¥½çš„å¹¶è¡Œç­–ç•¥
5. å®æ—¶è¿›åº¦ç›‘æ§

Author: Assistant
Date: 2024
"""

import os
import argparse
import torch
import soundfile as sf
from pathlib import Path
import glob
import subprocess
import tempfile
import shutil
import time
import json
import hashlib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import threading
from queue import Queue, Empty
import multiprocessing as mp
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

@dataclass
class VideoTask:
    """è§†é¢‘å¤„ç†ä»»åŠ¡æ•°æ®ç»“æ„"""
    video_path: str
    relative_path: str
    output_path: str
    file_hash: str
    file_size: int
    duration: Optional[float] = None
    status: str = "pending"  # pending, processing, completed, failed
    error_msg: str = ""
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    translation_preview: str = ""
    segments_count: int = 0

@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    completed: int = 0
    failed: int = 0
    skipped: int = 0
    segmented: int = 0
    total_duration: float = 0.0
    processing_time: float = 0.0

class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨ - è´Ÿè´£ä¿å­˜å’Œæ¢å¤å¤„ç†çŠ¶æ€"""
    
    def __init__(self, checkpoint_file: str):
        self.checkpoint_file = checkpoint_file
        self.tasks: Dict[str, VideoTask] = {}
        self.stats = ProcessingStats()
        self.lock = threading.Lock()
        
    def load_checkpoint(self) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if not os.path.exists(self.checkpoint_file):
            return False
            
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ¢å¤ä»»åŠ¡çŠ¶æ€
            for task_data in data.get('tasks', []):
                task = VideoTask(**task_data)
                self.tasks[task.file_hash] = task
                
            # æ¢å¤ç»Ÿè®¡ä¿¡æ¯
            stats_data = data.get('stats', {})
            self.stats = ProcessingStats(**stats_data)
            
            print(f"ğŸ“‚ Loaded checkpoint: {len(self.tasks)} tasks, {self.stats.completed} completed")
            return True
            
        except Exception as e:
            print(f"âš ï¸ Failed to load checkpoint: {e}")
            return False
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        with self.lock:
            try:
                checkpoint_dir = os.path.dirname(self.checkpoint_file)
                if checkpoint_dir:
                    os.makedirs(checkpoint_dir, exist_ok=True)
                
                data = {
                    'tasks': [asdict(task) for task in self.tasks.values()],
                    'stats': asdict(self.stats),
                    'timestamp': datetime.now().isoformat(),
                    'version': '2.0'
                }
                
                # åŸå­å†™å…¥
                temp_file = self.checkpoint_file + '.tmp'
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                
                if os.path.exists(self.checkpoint_file):
                    os.replace(temp_file, self.checkpoint_file)
                else:
                    os.rename(temp_file, self.checkpoint_file)
                    
            except Exception as e:
                print(f"âš ï¸ Failed to save checkpoint: {e}")
    
    def add_task(self, task: VideoTask):
        """æ·»åŠ ä»»åŠ¡"""
        with self.lock:
            self.tasks[task.file_hash] = task
            self.stats.total_files += 1
    
    def update_task(self, file_hash: str, **updates):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        with self.lock:
            if file_hash in self.tasks:
                task = self.tasks[file_hash]
                old_status = task.status
                
                for key, value in updates.items():
                    setattr(task, key, value)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if 'status' in updates:
                    new_status = updates['status']
                    if old_status != 'completed' and new_status == 'completed':
                        self.stats.completed += 1
                    elif old_status != 'failed' and new_status == 'failed':
                        self.stats.failed += 1
    
    def get_pending_tasks(self) -> List[VideoTask]:
        """è·å–å¾…å¤„ç†ä»»åŠ¡"""
        with self.lock:
            return [task for task in self.tasks.values() 
                   if task.status in ['pending', 'failed']]
    
    def get_progress_summary(self) -> Dict:
        """è·å–è¿›åº¦æ‘˜è¦"""
        with self.lock:
            total = len(self.tasks)
            if total == 0:
                return {'progress': 0, 'completed': 0, 'total': 0}
            
            completed = sum(1 for task in self.tasks.values() if task.status == 'completed')
            failed = sum(1 for task in self.tasks.values() if task.status == 'failed')
            processing = sum(1 for task in self.tasks.values() if task.status == 'processing')
            
            return {
                'progress': (completed / total) * 100,
                'completed': completed,
                'failed': failed,
                'processing': processing,
                'total': total,
                'remaining': total - completed - failed
            }

class GPUResourceManager:
    """GPUèµ„æºç®¡ç†å™¨"""
    
    def __init__(self, max_concurrent: int = 2):
        self.semaphore = threading.Semaphore(max_concurrent)
        self.usage_times = []
        self.lock = threading.Lock()
    
    def acquire(self):
        """è·å–GPUèµ„æº"""
        start_time = time.time()
        self.semaphore.acquire()
        return start_time
    
    def release(self, start_time: float):
        """é‡Šæ”¾GPUèµ„æº"""
        gpu_time = time.time() - start_time
        with self.lock:
            self.usage_times.append(gpu_time)
        self.semaphore.release()
        return gpu_time

class OptimizedModelManager:
    """ä¼˜åŒ–çš„æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, model_path: str, use_flash_attn: bool = False, save_audio: bool = False):
        self.model_path = model_path
        self.use_flash_attn = use_flash_attn
        self.save_audio = save_audio
        self._model = None
        self._processor = None
        self._model_lock = threading.Lock()
        self._load_time = 0
    
    def get_model(self):
        """è·å–æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._model is None:
            with self._model_lock:
                if self._model is None:  # Double-check locking
                    print(f"ğŸ¤– Loading optimized model from {self.model_path}...")
                    start_time = time.time()
                    
                    model_kwargs = {
                        "torch_dtype": torch.float16,  # ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
                        "device_map": "auto",
                        "low_cpu_mem_usage": True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                    }
                    
                    if self.use_flash_attn:
                        model_kwargs["attn_implementation"] = "flash_attention_2"
                    
                    self._model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                        self.model_path, **model_kwargs
                    )
                    self._processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
                    
                    if not self.save_audio:
                        self._model.disable_talker()
                    
                    # æ¨¡å‹é¢„çƒ­
                    self._warmup_model()
                    
                    self._load_time = time.time() - start_time
                    print(f"âœ… Model loaded in {self._load_time:.2f}s")
        
        return self._model, self._processor
    
    def _warmup_model(self):
        """æ¨¡å‹é¢„çƒ­"""
        try:
            print("ğŸ”¥ Warming up model...")
            dummy_text = "Hello, this is a warmup."
            inputs = self._processor(text=dummy_text, return_tensors="pt")
            inputs = inputs.to(self._model.device)
            
            with torch.no_grad():
                _ = self._model.generate(**inputs, max_new_tokens=5)
            
            print("âœ… Model warmup completed")
        except Exception as e:
            print(f"âš ï¸ Model warmup failed: {e}")

def calculate_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºå”¯ä¸€æ ‡è¯†"""
    hasher = hashlib.md5()
    hasher.update(file_path.encode('utf-8'))
    
    # æ·»åŠ æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
    try:
        stat = os.stat(file_path)
        hasher.update(str(stat.st_size).encode('utf-8'))
        hasher.update(str(stat.st_mtime).encode('utf-8'))
    except:
        pass
    
    return hasher.hexdigest()

def get_video_duration_fast(video_path: str) -> Optional[float]:
    """å¿«é€Ÿè·å–è§†é¢‘æ—¶é•¿"""
    try:
        cmd = [
            'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
            '-of', 'csv=p=0', video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        if result.returncode == 0 and result.stdout.strip():
            return float(result.stdout.strip())
    except Exception:
        pass
    return None

def batch_get_video_info(video_paths: List[str], max_workers: int = 8) -> Dict[str, Dict]:
    """æ‰¹é‡è·å–è§†é¢‘ä¿¡æ¯"""
    print(f"ğŸ“Š Analyzing {len(video_paths)} videos...")
    
    def get_single_info(video_path):
        try:
            stat = os.stat(video_path)
            duration = get_video_duration_fast(video_path)
            return video_path, {
                'size': stat.st_size,
                'duration': duration,
                'mtime': stat.st_mtime
            }
        except Exception as e:
            return video_path, {'error': str(e)}
    
    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(get_single_info, path): path for path in video_paths}
        
        for future in as_completed(futures):
            video_path, info = future.result()
            results[video_path] = info
    
    return results

def split_video_optimized(video_path: str, segment_duration: int = 10, 
                         overlap_duration: int = 2) -> Tuple[List[Tuple[str, float, float]], str]:
    """ä¼˜åŒ–çš„è§†é¢‘åˆ†å‰²"""
    duration = get_video_duration_fast(video_path)
    if duration is None or duration <= segment_duration:
        return [], ""
    
    temp_dir = tempfile.mkdtemp(prefix="video_segments_opt_")
    segments = []
    
    # è®¡ç®—æ‰€æœ‰åˆ†å‰²ç‚¹
    segment_info = []
    start_time = 0
    segment_count = 0
    
    while start_time < duration:
        end_time = min(start_time + segment_duration, duration)
        actual_duration = end_time - start_time
        
        if actual_duration < 2:
            break
        
        segment_info.append((segment_count, start_time, end_time, actual_duration))
        start_time = end_time - overlap_duration
        if start_time >= duration - overlap_duration:
            break
        segment_count += 1
    
    print(f"ğŸ”ª Creating {len(segment_info)} segments in parallel...")
    
    # å¹¶è¡Œåˆ›å»ºåˆ†å‰²
    def create_segment(info):
        segment_count, start_time, end_time, actual_duration = info
        segment_filename = f"segment_{segment_count:04d}.mp4"
        segment_path = os.path.join(temp_dir, segment_filename)
        
        cmd = [
            'ffmpeg', '-y', '-i', video_path,
            '-ss', str(start_time), '-t', str(actual_duration),
            '-c:v', 'libx264', '-preset', 'ultrafast',  # æœ€å¿«ç¼–ç 
            '-c:a', 'aac', '-avoid_negative_ts', 'make_zero',
            segment_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            if result.returncode == 0 and os.path.exists(segment_path):
                return (segment_path, start_time, end_time)
        except Exception as e:
            print(f"âŒ Failed to create segment {segment_count}: {e}")
        
        return None
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œåˆ†å‰²
    with ProcessPoolExecutor(max_workers=min(4, len(segment_info))) as executor:
        futures = {executor.submit(create_segment, info): info for info in segment_info}
        
        for future in as_completed(futures):
            result = future.result()
            if result:
                segments.append(result)
    
    segments.sort(key=lambda x: x[1])  # æŒ‰å¼€å§‹æ—¶é—´æ’åº
    print(f"âœ… Created {len(segments)} segments successfully")
    
    return segments, temp_dir



def process_single_video_optimized(task: VideoTask, 
                                 model_manager: OptimizedModelManager,
                                 gpu_manager: GPUResourceManager,
                                 progress_manager: ProgressManager,
                                 segment_duration: int = 10,
                                 overlap_duration: int = 2,
                                 disable_segmentation: bool = False,
                                 worker_id: int = 0) -> bool:
    """ä¼˜åŒ–çš„å•è§†é¢‘å¤„ç†å‡½æ•°"""
    
    try:
        start_time = time.time()
        progress_manager.update_task(task.file_hash, 
                                   status='processing', 
                                   start_time=start_time)
        
        print(f"ğŸ”„ Worker {worker_id}: Processing {os.path.basename(task.video_path)}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
        should_segment = (not disable_segmentation and 
                         task.duration and 
                         task.duration > segment_duration)
        
        if not should_segment:
            # ç›´æ¥å¤„ç†æ•´ä¸ªè§†é¢‘
            print(f"ğŸ“¹ Worker {worker_id}: Processing video directly")
            translation = translate_single_video(task.video_path, model_manager, gpu_manager)
        else:
            # åˆ†å‰²å¤„ç†
            print(f"ğŸ”ª Worker {worker_id}: Video requires segmentation ({task.duration:.1f}s)")
            translation = translate_segmented_video(task.video_path, model_manager, gpu_manager,
                                                   segment_duration, overlap_duration)
            task.segments_count = translation.count('\n\n') + 1 if translation else 0
        
        # ä¿å­˜ç¿»è¯‘ç»“æœ
        if translation and translation.strip():
            os.makedirs(os.path.dirname(task.output_path), exist_ok=True)
            
            with open(task.output_path, "w", encoding="utf-8") as f:
                f.write(translation)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            progress_manager.update_task(task.file_hash,
                                       status='completed',
                                       end_time=end_time,
                                       translation_preview=translation[:200],
                                       segments_count=getattr(task, 'segments_count', 0))
            
            print(f"âœ… Worker {worker_id}: Completed {task.relative_path} in {processing_time:.2f}s")
            return True
        else:
            raise ValueError("Empty translation result")
            
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Worker {worker_id}: Failed {task.relative_path}: {error_msg}")
        
        progress_manager.update_task(task.file_hash,
                                   status='failed',
                                   error_msg=error_msg,
                                   end_time=time.time())
        return False

def translate_single_video(video_path: str, model_manager: OptimizedModelManager, 
                          gpu_manager: GPUResourceManager) -> str:
    """ç¿»è¯‘å•ä¸ªè§†é¢‘"""
    model, processor = model_manager.get_model()
    
    conversation = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "video", "video": video_path},
                {"type": "text", "text": "ç¿»è¯‘æä¾›çš„è§†é¢‘ä¸­çš„è¯´è¯å†…å®¹åˆ°ä¸­æ–‡ã€‚åªéœ€è¦è¾“å‡ºç¿»è¯‘å†…å®¹åŸæ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚"}
            ]
        }
    ]
    
    gpu_start = gpu_manager.acquire()
    try:
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(conversation, use_audio_in_video=True)
        inputs = processor(text=text, audio=audios, images=images, videos=videos,
                         return_tensors="pt", padding=True, use_audio_in_video=True)
        inputs = inputs.to(model.device).to(model.dtype)
        
        with torch.no_grad():
            text_ids = model.generate(**inputs, use_audio_in_video=True,
                                    return_audio=False, max_new_tokens=512)
        
        translation = processor.batch_decode(text_ids, skip_special_tokens=True,
                                           clean_up_tokenization_spaces=False)[0]
        return translation.strip()
        
    finally:
        gpu_manager.release(gpu_start)


def translate_segmented_video(video_path: str, model_manager: OptimizedModelManager,
                            gpu_manager: GPUResourceManager, segment_duration: int,
                            overlap_duration: int) -> str:
    """ç¿»è¯‘åˆ†æ®µè§†é¢‘"""
    segments, temp_dir = split_video_optimized(video_path, segment_duration, overlap_duration)
    
    try:
        if not segments:
            raise ValueError("Failed to create video segments")
        
        model, processor = model_manager.get_model()
        translations = []
        
        # å¤„ç†æ¯ä¸ªåˆ†æ®µ
        for i, (segment_path, start_time, end_time) in enumerate(segments):
            print(f"  ğŸ¬ Processing segment {i+1}/{len(segments)}: {start_time:.1f}s - {end_time:.1f}s")
            
            conversation = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "video": segment_path},
                        {"type": "text", "text": "ç¿»è¯‘æä¾›çš„è§†é¢‘ä¸­çš„è¯´è¯å†…å®¹åˆ°ä¸­æ–‡ã€‚åªéœ€è¦è¾“å‡ºç¿»è¯‘å†…å®¹åŸæ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚"}
                    ]
                }
            ]
            
            gpu_start = gpu_manager.acquire()
            try:
                text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
                audios, images, videos = process_mm_info(conversation, use_audio_in_video=True)
                inputs = processor(text=text, audio=audios, images=images, videos=videos,
                                 return_tensors="pt", padding=True, use_audio_in_video=True)
                inputs = inputs.to(model.device).to(model.dtype)
                
                with torch.no_grad():
                    text_ids = model.generate(**inputs, use_audio_in_video=True,
                                            return_audio=False, max_new_tokens=512)
                
                translation = processor.batch_decode(text_ids, skip_special_tokens=True,
                                                   clean_up_tokenization_spaces=False)[0]
                
                if translation and translation.strip():
                    translations.append(translation.strip())
                    print(f"    âœ… Segment {i+1} completed: {translation[:50]}...")
                
            finally:
                gpu_manager.release(gpu_start)
        
        # åˆå¹¶ç¿»è¯‘ç»“æœ
        if translations:
            return "\n\n".join(translations)
        else:
            raise ValueError("No segments were successfully translated")
            
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"âš ï¸ Failed to cleanup {temp_dir}: {e}")

def get_video_files_recursive(folder_path: str, max_depth: int = 10) -> List[Tuple[str, str]]:
    """é€’å½’è·å–è§†é¢‘æ–‡ä»¶"""
    folder_path = Path(folder_path)
    if not folder_path.exists():
        raise FileNotFoundError(f"Folder not found: {folder_path}")
    
    video_extensions = [
        '*.mp4', '*.MP4', '*.avi', '*.AVI', '*.mov', '*.MOV', 
        '*.mkv', '*.MKV', '*.wmv', '*.WMV', '*.flv', '*.FLV',
        '*.webm', '*.WEBM', '*.m4v', '*.M4V', '*.3gp', '*.3GP'
    ]
    
    video_files = []
    
    def _recursive_search(current_path, current_depth):
        if current_depth > max_depth:
            return
        
        for pattern in video_extensions:
            for video_file in current_path.glob(pattern):
                if video_file.is_file():
                    relative_path = video_file.relative_to(folder_path)
                    video_files.append((str(video_file), str(relative_path)))
        
        for subdir in current_path.iterdir():
            if subdir.is_dir():
                _recursive_search(subdir, current_depth + 1)
    
    print(f"ğŸ” Scanning for video files in: {folder_path}")
    _recursive_search(folder_path, 0)
    
    video_files.sort(key=lambda x: x[1])
    print(f"ğŸ“ Found {len(video_files)} video files")
    
    return video_files

def run_optimized_parallel_translation(video_files_info: List[Tuple[str, str]], 
                                     output_folder: str,
                                     model_path: str,
                                     checkpoint_file: str,
                                     preserve_structure: bool = True,
                                     segment_duration: int = 10,
                                     overlap_duration: int = 2,
                                     disable_segmentation: bool = False,
                                     parallel_workers: int = 5,
                                     max_concurrent_gpu: int = 2,
                                     use_flash_attn: bool = False,
                                     save_audio: bool = False) -> Dict:
    """è¿è¡Œä¼˜åŒ–çš„å¹¶è¡Œç¿»è¯‘"""
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    progress_manager = ProgressManager(checkpoint_file)
    gpu_manager = GPUResourceManager(max_concurrent_gpu)
    model_manager = OptimizedModelManager(model_path, use_flash_attn, save_audio)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    progress_manager.load_checkpoint()
    
    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
    print("ğŸ“‹ Preparing task list...")
    
    # æ‰¹é‡è·å–è§†é¢‘ä¿¡æ¯
    video_paths = [video_file for video_file, _ in video_files_info]
    video_info_dict = batch_get_video_info(video_paths)
    
    new_tasks = 0
    for video_file, relative_path in video_files_info:
        file_hash = calculate_file_hash(video_file)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if file_hash in progress_manager.tasks:
            existing_task = progress_manager.tasks[file_hash]
            if existing_task.status == 'completed' and os.path.exists(existing_task.output_path):
                continue  # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
        
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        if preserve_structure:
            video_path = Path(relative_path)
            output_dir = Path(output_folder) / video_path.parent
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / f"{video_path.stem}.txt"
        else:
            flat_name = str(Path(relative_path).with_suffix('')).replace(os.sep, '_').replace('/', '_')
            output_path = Path(output_folder) / f"{flat_name}.txt"
        
        # è·å–è§†é¢‘ä¿¡æ¯
        info = video_info_dict.get(video_file, {})
        
        task = VideoTask(
            video_path=video_file,
            relative_path=relative_path,
            output_path=str(output_path),
            file_hash=file_hash,
            file_size=info.get('size', 0),
            duration=info.get('duration')
        )
        
        progress_manager.add_task(task)
        new_tasks += 1
    
    print(f"ğŸ“Š Task summary: {new_tasks} new tasks added")
    
    # è·å–å¾…å¤„ç†ä»»åŠ¡
    pending_tasks = progress_manager.get_pending_tasks()
    if not pending_tasks:
        print("âœ… All tasks completed!")
        return progress_manager.get_progress_summary()
    
    print(f"ğŸš€ Starting optimized parallel processing: {len(pending_tasks)} tasks")
    
    # è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹çš„å®šæ—¶å™¨
    def auto_save_checkpoint():
        while True:
            time.sleep(30)  # æ¯30ç§’ä¿å­˜ä¸€æ¬¡
            progress_manager.save_checkpoint()
    
    checkpoint_thread = threading.Thread(target=auto_save_checkpoint, daemon=True)
    checkpoint_thread.start()
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=parallel_workers) as executor:
        futures = {}
        
        for i, task in enumerate(pending_tasks):
            future = executor.submit(
                process_single_video_optimized,
                task, model_manager, gpu_manager, progress_manager,
                segment_duration, overlap_duration, disable_segmentation, i + 1
            )
            futures[future] = task
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        completed_count = 0
        for future in as_completed(futures):
            task = futures[future]
            success = future.result()
            completed_count += 1
            
            # å®æ—¶è¿›åº¦æ˜¾ç¤º
            progress = progress_manager.get_progress_summary()
            print(f"ğŸ“Š Progress: {progress['completed']}/{progress['total']} "
                  f"({progress['progress']:.1f}%) - "
                  f"Failed: {progress['failed']}, "
                  f"Remaining: {progress['remaining']}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if completed_count % 5 == 0:
                progress_manager.save_checkpoint()
    
    # æœ€ç»ˆä¿å­˜
    progress_manager.save_checkpoint()
    
    # è¿”å›æœ€ç»ˆç»Ÿè®¡
    final_stats = progress_manager.get_progress_summary()
    print(f"\nğŸ‰ Optimized parallel processing completed!")
    print(f"âœ… Success: {final_stats['completed']}")
    print(f"âŒ Failed: {final_stats['failed']}")
    print(f"ğŸ“Š Success rate: {final_stats['completed']/final_stats['total']*100:.1f}%")
    
    return final_stats

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Optimized Qwen2.5-Omni Parallel Video Translation")
    
    # è¾“å…¥é€‰é¡¹
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--video_path", type=str, help="Single video file path")
    input_group.add_argument("--input_folder", type=str, help="Input folder containing videos")
    
    parser.add_argument("--output_path", type=str, help="Output path or folder")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-Omni-7B", help="Model path")
    parser.add_argument("--checkpoint_file", type=str, default="./progress_checkpoint.json", 
                       help="Checkpoint file for resume capability")
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument("--preserve_structure", action="store_true", help="Preserve directory structure")
    parser.add_argument("--segment_duration", type=int, default=10, help="Segment duration (seconds)")
    parser.add_argument("--overlap_duration", type=int, default=2, help="Overlap duration (seconds)")
    parser.add_argument("--disable_segmentation", action="store_true", help="Disable video segmentation")
    
    # å¹¶è¡Œå¤„ç†é€‰é¡¹
    parser.add_argument("--parallel_workers", type=int, default=5, help="Number of parallel workers")
    parser.add_argument("--max_concurrent_gpu", type=int, default=2, help="Max concurrent GPU operations")
    
    # æ¨¡å‹é€‰é¡¹
    parser.add_argument("--use_flash_attn", action="store_true", help="Use Flash Attention 2")
    parser.add_argument("--save_audio", action="store_true", help="Save audio output")
    parser.add_argument("--use_audio", action="store_true", help="Use audio in video")
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--max_depth", type=int, default=10, help="Max recursion depth")
    parser.add_argument("--resume", action="store_true", help="Resume from checkpoint")
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('optimized_translation.log'),
            logging.StreamHandler()
        ]
    )
    
    try:
        if args.video_path:
            # å•æ–‡ä»¶å¤„ç†
            if not os.path.exists(args.video_path):
                raise FileNotFoundError(f"Video file not found: {args.video_path}")
            
            output_path = args.output_path or "./evaluation/test_data/optimized_result.txt"
            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
            
            # åˆ›å»ºå•ä¸ªä»»åŠ¡
            file_hash = calculate_file_hash(args.video_path)
            duration = get_video_duration_fast(args.video_path)
            
            task = VideoTask(
                video_path=args.video_path,
                relative_path=os.path.basename(args.video_path),
                output_path=output_path,
                file_hash=file_hash,
                file_size=os.path.getsize(args.video_path),
                duration=duration
            )
            
            # åˆå§‹åŒ–ç®¡ç†å™¨
            progress_manager = ProgressManager(args.checkpoint_file)
            gpu_manager = GPUResourceManager(1)
            model_manager = OptimizedModelManager(args.model_path, args.use_flash_attn, args.save_audio)
            
            # å¤„ç†å•ä¸ªè§†é¢‘
            success = process_single_video_optimized(
                task, model_manager, gpu_manager, progress_manager,
                args.segment_duration, args.overlap_duration, args.disable_segmentation, 1
            )
            
            if success:
                with open(output_path, 'r', encoding='utf-8') as f:
                    translation = f.read()
                print(f"âœ… Translation completed and saved to {output_path}")
                print(f"ğŸ“ Translation: {translation[:200]}...")
            else:
                print("âŒ Translation failed")
        
        else:
            # æ‰¹é‡å¤„ç†
            video_files_info = get_video_files_recursive(args.input_folder, args.max_depth)
            
            if not video_files_info:
                print("No video files found.")
                return
            
            output_folder = args.output_path or "./evaluation/test_data/optimized_results"
            os.makedirs(output_folder, exist_ok=True)
            
            # è¿è¡Œä¼˜åŒ–çš„å¹¶è¡Œç¿»è¯‘
            stats = run_optimized_parallel_translation(
                video_files_info, output_folder, args.model_path, args.checkpoint_file,
                args.preserve_structure, args.segment_duration, args.overlap_duration,
                args.disable_segmentation, args.parallel_workers, args.max_concurrent_gpu,
                args.use_flash_attn, args.save_audio
            )
            
            print(f"ğŸ¯ Final Results: {stats}")
    
    except KeyboardInterrupt:
        print("\nâ¸ï¸ Processing interrupted by user")
    except Exception as e:
        print(f"âŒ Fatal error: {e}")
        raise

if __name__ == "__main__":
    main() 