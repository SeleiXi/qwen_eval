import os
import argparse
import torch
import soundfile as sf
from pathlib import Path
import glob
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

# if you want to run this file, torch is required to be at least 2.6.0.


def parse_args():
    parser = argparse.ArgumentParser(description="Qwen2.5-Omni Video Translation (Reverse Order with Batch Processing)")
    
    # Input options - either single video or folder
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--video_path", type=str, help="Path to a single video file")
    input_group.add_argument("--input_folder", type=str, help="Path to folder containing video files for batch processing")
    
    parser.add_argument("--output_path", type=str, help="Path to save the translation text (for single file) or output folder (for batch processing)")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-Omni-7B", help="Path to the model")
    parser.add_argument("--source_lang", type=str, default="auto", help="Source language (auto for auto-detection)")
    parser.add_argument("--target_lang", type=str, default="en", help="Target language (en, zh, etc.)")
    parser.add_argument("--use_audio", action="store_true", help="Use audio in video for better translation")
    parser.add_argument("--save_audio", action="store_true", help="Save audio output")
    parser.add_argument("--use_flash_attn", action="store_true", help="Use Flash Attention 2 for faster inference")
    
    # Parallel processing parameters
    parser.add_argument("--parallel_workers", type=int, default=5, help="Number of parallel video processing workers (default: 5)")
    parser.add_argument("--shared_model", action="store_true", help="Use shared model for all workers (saves memory)")
    parser.add_argument("--max_concurrent_gpu", type=int, default=2, help="Maximum concurrent GPU operations (default: 2)")
    parser.add_argument("--serial_mode", action="store_true", help="Use serial processing instead of parallel (original behavior)")
    
    return parser.parse_args()

def get_video_files_reverse(folder_path):
    """
    Get all video files from the specified folder in REVERSE alphabetical order
    
    Args:
        folder_path: Path to the folder containing video files
        
    Returns:
        List of video file paths in reverse order (z-a, 9-0)
    """
    folder_path = Path(folder_path)
    if not folder_path.exists():
        raise FileNotFoundError(f"Folder not found: {folder_path}")
    
    # Find all video files (common formats, case insensitive)
    video_extensions = [
        '*.mp4', '*.MP4', '*.Mp4', '*.mP4',
        '*.avi', '*.AVI', '*.Avi', '*.aVi',
        '*.mov', '*.MOV', '*.Mov', '*.mOv',
        '*.mkv', '*.MKV', '*.Mkv', '*.mKv',
        '*.wmv', '*.WMV', '*.Wmv', '*.wMv',
        '*.flv', '*.FLV', '*.Flv', '*.fLv',
        '*.webm', '*.WEBM', '*.Webm', '*.wEbm',
        '*.m4v', '*.M4V', '*.M4v', '*.m4V',
        '*.3gp', '*.3GP', '*.3Gp', '*.3gP'
    ]
    
    video_files = []
    for pattern in video_extensions:
        video_files.extend(glob.glob(str(folder_path / pattern)))
    
    if not video_files:
        print(f"Warning: No video files found in folder: {folder_path}")
        print("Supported formats: mp4, avi, mov, mkv, wmv, flv, webm, m4v, 3gp")
        return []
    
    # ğŸ”„ REVERSE ORDER: Sort in descending order (z-a, 9-0)
    return sorted(video_files, reverse=True)

# Global model management for parallel processing
_shared_model = None
_shared_processor = None
_model_lock = threading.Lock()
_gpu_semaphore = None

def get_shared_model(model_path, use_flash_attn=False, save_audio=False):
    """
    è·å–å…±äº«æ¨¡å‹å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    Args:
        model_path: Path to the model
        use_flash_attn: Whether to use Flash Attention
        save_audio: Whether to save audio output
        
    Returns:
        tuple: (model, processor)
    """
    global _shared_model, _shared_processor
    
    with _model_lock:
        if _shared_model is None:
            print(f"ğŸ¤– Loading shared model from {model_path}...")
            model_kwargs = {
                "torch_dtype": "auto",
                "device_map": "auto",
            }
            
            if use_flash_attn:
                model_kwargs["attn_implementation"] = "flash_attention_2"
            
            _shared_model = Qwen2_5OmniForConditionalGeneration.from_pretrained(model_path, **model_kwargs)
            _shared_processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
            
            if not save_audio:
                _shared_model.disable_talker()
            
            print("âœ… Shared model loaded successfully")
    
    return _shared_model, _shared_processor

def translate_video(video_path, model_path, source_lang="auto", target_lang="en", 
                   use_audio=True, save_audio=False, use_flash_attn=False):
    """
    Translate content from a video using Qwen2.5-Omni model
    
    Args:
        video_path: Path to the video file
        model_path: Path to the Qwen2.5-Omni model
        source_lang: Source language (auto for auto-detection)
        target_lang: Target language for translation
        use_audio: Whether to use audio in video
        save_audio: Whether to save audio output
        use_flash_attn: Whether to use Flash Attention 2
        
    Returns:
        Translation text
    """
    # Load model and processor
    print(f"Loading model from {model_path}...")
    model_kwargs = {
        "torch_dtype": "auto",
        "device_map": "auto",
    }
    
    if use_flash_attn:
        model_kwargs["attn_implementation"] = "flash_attention_2"
    
    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(model_path, **model_kwargs)
    processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
    
    if not save_audio:
        model.disable_talker()
    
    # Prepare conversation with translation instruction
    conversation = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
            ],
        },
        {
            "role": "user",
            "content": [
                {"type": "video", "video": video_path},
                {"type": "text", "text": f"ç¿»è¯‘æä¾›çš„è§†é¢‘ä¸­çš„è¯´è¯å†…å®¹åˆ°ä¸­æ–‡ã€‚åªéœ€è¦è¾“å‡ºç¿»è¯‘å†…å®¹åŸæ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚"}
            ],
        },
    ]

    # Process the input
    print("Processing video...")
    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
    audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_audio)
    inputs = processor(text=text, audio=audios, images=images, videos=videos, 
                      return_tensors="pt", padding=True, use_audio_in_video=use_audio)
    inputs = inputs.to(model.device).to(model.dtype)

    # Generate translation
    print("Generating translation...")
    if save_audio:
        print("saving audio")
        text_ids, audio = model.generate(**inputs, use_audio_in_video=use_audio)
    else:
        text_ids = model.generate(**inputs, use_audio_in_video=use_audio, return_audio=False)
    
    # Decode translation
    print("decoding")
    translation = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    
    # Save audio if requested
    if save_audio:
        print("saving audio")
        audio_output_path = os.path.splitext(video_path)[0] + "_translation.wav"
        sf.write(
            audio_output_path,
            audio.reshape(-1).detach().cpu().numpy(),
            samplerate=24000,
        )
        print(f"Audio saved to {audio_output_path}")
    
    return translation

def translate_video_worker(video_path, model_path, source_lang="auto", target_lang="en", 
                          use_audio=True, save_audio=False, use_flash_attn=False, 
                          shared_model=False, worker_id=0):
    """
    å·¥ä½œçº¿ç¨‹çš„è§†é¢‘ç¿»è¯‘å‡½æ•°
    
    Args:
        video_path: Path to the video file
        model_path: Path to the Qwen2.5-Omni model
        source_lang: Source language (auto for auto-detection)
        target_lang: Target language for translation
        use_audio: Whether to use audio in video
        save_audio: Whether to save audio output
        use_flash_attn: Whether to use Flash Attention 2
        shared_model: Whether to use shared model
        worker_id: Worker ID for logging
        
    Returns:
        Translation text
    """
    global _gpu_semaphore
    
    try:
        print(f"ğŸ”„ Worker {worker_id}: Starting translation for {os.path.basename(video_path)}")
        
        # è·å–æ¨¡å‹
        if shared_model:
            model, processor = get_shared_model(model_path, use_flash_attn, save_audio)
        else:
            # ç‹¬ç«‹åŠ è½½æ¨¡å‹
            print(f"ğŸ¤– Worker {worker_id}: Loading independent model...")
            model_kwargs = {
                "torch_dtype": "auto",
                "device_map": "auto",
            }
            
            if use_flash_attn:
                model_kwargs["attn_implementation"] = "flash_attention_2"
            
            model = Qwen2_5OmniForConditionalGeneration.from_pretrained(model_path, **model_kwargs)
            processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
            
            if not save_audio:
                model.disable_talker()
        
        # å‡†å¤‡å¯¹è¯
        conversation = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": f"ç¿»è¯‘æä¾›çš„è§†é¢‘ä¸­çš„è¯´è¯å†…å®¹åˆ°ä¸­æ–‡ã€‚åªéœ€è¦è¾“å‡ºç¿»è¯‘å†…å®¹åŸæ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚"}
                ],
            },
        ]

        # å¤„ç†è¾“å…¥
        print(f"ğŸ“Š Worker {worker_id}: Processing video...")
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_audio)
        inputs = processor(text=text, audio=audios, images=images, videos=videos, 
                          return_tensors="pt", padding=True, use_audio_in_video=use_audio)
        inputs = inputs.to(model.device).to(model.dtype)

        # GPUå¹¶å‘æ§åˆ¶
        if _gpu_semaphore:
            _gpu_semaphore.acquire()
        
        try:
            # ç”Ÿæˆç¿»è¯‘
            print(f"ğŸ¤– Worker {worker_id}: Generating translation...")
            with torch.no_grad():  # èŠ‚çœå†…å­˜
                if save_audio:
                    text_ids, audio = model.generate(**inputs, use_audio_in_video=use_audio, max_new_tokens=512)
                    # Save audio for this worker
                    audio_output_path = os.path.splitext(video_path)[0] + f"_worker{worker_id}_translation.wav"
                    sf.write(
                        audio_output_path,
                        audio.reshape(-1).detach().cpu().numpy(),
                        samplerate=24000,
                    )
                    print(f"ğŸµ Worker {worker_id}: Audio saved to {audio_output_path}")
                else:
                    text_ids = model.generate(**inputs, use_audio_in_video=use_audio, return_audio=False, max_new_tokens=512)
        finally:
            if _gpu_semaphore:
                _gpu_semaphore.release()
        
        # è§£ç ç¿»è¯‘
        print(f"ğŸ“ Worker {worker_id}: Decoding...")
        translation = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        
        # æ¸…ç†ç‹¬ç«‹æ¨¡å‹
        if not shared_model:
            del model, processor
            torch.cuda.empty_cache()
        
        print(f"âœ… Worker {worker_id}: Completed translation for {os.path.basename(video_path)}")
        return translation.strip()
        
    except Exception as e:
        print(f"âŒ Worker {worker_id}: Error translating {video_path}: {str(e)}")
        return ""

def reverse_parallel_batch_translate(video_files, output_folder, model_path, source_lang="auto", target_lang="en",
                                    use_audio=True, save_audio=False, use_flash_attn=False,
                                    parallel_workers=5, shared_model=False, max_concurrent_gpu=2):
    """
    åå‘é¡ºåºå¹¶è¡Œæ‰¹é‡ç¿»è¯‘è§†é¢‘æ–‡ä»¶
    
    Args:
        video_files: List of video file paths (already in reverse order)
        output_folder: Output folder for translations
        model_path: Path to the model
        source_lang: Source language
        target_lang: Target language
        use_audio: Whether to use audio
        save_audio: Whether to save audio
        use_flash_attn: Whether to use Flash Attention
        parallel_workers: Number of parallel workers
        shared_model: Whether to use shared model
        max_concurrent_gpu: Maximum concurrent GPU operations
        
    Returns:
        Dict with processing statistics
    """
    global _gpu_semaphore
    
    # åˆå§‹åŒ–GPUä¿¡å·é‡
    _gpu_semaphore = threading.Semaphore(max_concurrent_gpu)
    
    # é¢„åŠ è½½å…±äº«æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if shared_model:
        get_shared_model(model_path, use_flash_attn, save_audio)
    
    # å‡†å¤‡ä»»åŠ¡é˜Ÿåˆ—
    tasks = []
    for video_file in video_files:
        video_name = Path(video_file).stem
        output_filename = f"{video_name}.txt"
        output_path = os.path.join(output_folder, output_filename)
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            print(f"â­ï¸ Skipping existing: {os.path.basename(video_file)}")
            continue
        
        tasks.append((video_file, output_path))
    
    if not tasks:
        print("No new videos to process.")
        return {"skipped": len(video_files), "successful": 0, "failed": 0}
    
    print(f"ğŸš€ Starting REVERSE ORDER parallel processing of {len(tasks)} videos with {parallel_workers} workers")
    print(f"ğŸ“Š Configuration: Shared Model = {shared_model}, Max GPU Concurrent = {max_concurrent_gpu}")
    print(f"ğŸ”„ Processing order: REVERSE (z-a, 9-0)")
    
    # ç»Ÿè®¡è®¡æ•°å™¨
    successful_count = 0
    failed_count = 0
    skipped_count = len(video_files) - len(tasks)
    
    start_time = time.time()
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œç¿»è¯‘
    with ThreadPoolExecutor(max_workers=parallel_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_video = {}
        for i, (video_file, output_path) in enumerate(tasks):
            future = executor.submit(
                translate_video_worker,
                video_file, model_path, source_lang, target_lang,
                use_audio, save_audio, use_flash_attn, shared_model, i+1
            )
            future_to_video[future] = (video_file, output_path)
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_video):
            video_file, output_path = future_to_video[future]
            
            try:
                translation = future.result()
                
                if translation and translation.strip():
                    # ä¿å­˜ç¿»è¯‘ç»“æœ
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(translation)
                    
                    print(f"âœ… Saved: {os.path.basename(output_path)}")
                    print(f"ğŸ“ Preview: {translation[:50]}...")
                    successful_count += 1
                else:
                    print(f"âš ï¸ Empty translation for: {os.path.basename(video_file)}")
                    failed_count += 1
                    
            except Exception as e:
                print(f"âŒ Task failed for {os.path.basename(video_file)}: {e}")
                failed_count += 1
    
    elapsed_time = time.time() - start_time
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*80}")
    print(f"ğŸ‰ REVERSE ORDER PARALLEL PROCESSING COMPLETED!")
    print(f"{'='*80}")
    print(f"â±ï¸ Total processing time: {elapsed_time/60:.1f} minutes")
    print(f"ğŸ“Š Processing Summary:")
    print(f"  ğŸ“ Total files found: {len(video_files)}")
    print(f"  ğŸ”„ Processing order: REVERSE (z-a, 9-0)")
    print(f"  â­ï¸ Files skipped (already exist): {skipped_count}")
    print(f"  âœ… Files successfully processed: {successful_count}")
    print(f"  âŒ Files failed to process: {failed_count}")
    print(f"  ğŸš€ Average speed: {successful_count*60/elapsed_time:.1f} videos/hour")
    print(f"  âš¡ Parallel efficiency: {parallel_workers}x workers")
    print(f"  ğŸ“‚ Results saved in: {output_folder}")
    print(f"{'='*80}")
    
    return {
        "successful": successful_count,
        "failed": failed_count,
        "skipped": skipped_count,
        "total_time": elapsed_time
    }

def main():
    args = parse_args()
    
    if args.video_path:
        # Single video processing
        if not os.path.exists(args.video_path):
            raise FileNotFoundError(f"Video file not found: {args.video_path}")
        
        # Set default output path if not provided
        if not args.output_path:
            args.output_path = "./evaluation/test_data/qwen_result.txt"
        
        # Create output directory if needed
        os.makedirs(os.path.dirname(os.path.abspath(args.output_path)), exist_ok=True)
        
        # Translate video
        translation = translate_video(
            args.video_path, 
            args.model_path, 
            args.source_lang, 
            args.target_lang,
            args.use_audio,
            args.save_audio,
            args.use_flash_attn
        )
        
        # Save translation to file
        with open(args.output_path, "w", encoding="utf-8") as f:
            f.write(translation)
        
        print(f"Translation saved to {args.output_path}")
        print("\nTranslation result:")
        print("-" * 50)
        print(translation)
        print("-" * 50)
        
    else:
        # Batch processing with REVERSE ORDER
        print(f"ğŸ”„ Starting REVERSE ORDER batch processing for folder: {args.input_folder}")
        
        # ğŸ”„ Use reverse order function
        video_files = get_video_files_reverse(args.input_folder)
        
        if not video_files:
            print("No video files found to process. Exiting.")
            return
            
        print(f"ğŸ“ Found {len(video_files)} video files to process in REVERSE order")
        print(f"ğŸ”„ Processing order: {[os.path.basename(f) for f in video_files[:5]]}{'...' if len(video_files) > 5 else ''}")
        
        # Set default output folder if not provided
        if not args.output_path:
            args.output_path = "./evaluation/test_data/reverse_batch_results"
        
        # Create output directory
        os.makedirs(args.output_path, exist_ok=True)
        
        if args.serial_mode:
            # åŸå§‹ä¸²è¡Œå¤„ç†æ¨¡å¼
            print("ğŸ”— Using SERIAL processing mode (original behavior)")
            
            # Initialize counters for statistics
            skipped_count = 0
            successful_count = 0
            failed_count = 0
            start_time = time.time()
            
            # Process each video file in REVERSE order
            for i, video_file in enumerate(video_files, 1):
                print(f"\n{'='*60}")
                print(f"ğŸ”„ Processing file {i}/{len(video_files)} (REVERSE ORDER): {os.path.basename(video_file)}")
                print(f"{'='*60}")
                
                # Generate output filename based on input filename
                video_name = Path(video_file).stem
                output_filename = f"{video_name}.txt"
                output_path = os.path.join(args.output_path, output_filename)
                
                # Check if output file already exists
                if os.path.exists(output_path):
                    print(f"â­ï¸  Skipping: Translation file already exists at {output_path}")
                    skipped_count += 1
                    continue
                
                try:
                    translation = translate_video(
                        video_file,
                        args.model_path,
                        args.source_lang,
                        args.target_lang,
                        args.use_audio,
                        args.save_audio,
                        args.use_flash_attn
                    )
                    
                    # Save translation to file
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(translation)
                    
                    print(f"âœ“ Translation saved to {output_path}")
                    print(f"Translation preview: {translation[:100]}...")
                    successful_count += 1
                    
                except Exception as e:
                    print(f"âœ— Error processing {video_file}: {str(e)}")
                    failed_count += 1
                    continue
            
            elapsed_time = time.time() - start_time
            print(f"\n{'='*60}")
            print(f"ğŸ”„ REVERSE ORDER SERIAL PROCESSING COMPLETED!")
            print(f"{'='*60}")
            print(f"â±ï¸ Total processing time: {elapsed_time/60:.1f} minutes")
            print(f"ğŸ“Š Processing Summary:")
            print(f"  ğŸ“ Total files found: {len(video_files)}")
            print(f"  ğŸ”„ Processing order: REVERSE (z-a, 9-0)")
            print(f"  ğŸ”— Processing mode: SERIAL")
            print(f"  â­ï¸  Files skipped (already exist): {skipped_count}")
            print(f"  âœ… Files successfully processed: {successful_count}")
            print(f"  âŒ Files failed to process: {failed_count}")
            print(f"  ğŸ“‚ Results saved in: {args.output_path}")
            print(f"{'='*60}")
        else:
            # æ–°çš„å¹¶è¡Œå¤„ç†æ¨¡å¼
            print("âš¡ Using PARALLEL processing mode (new feature)")
            
            # æ‰§è¡Œåå‘é¡ºåºå¹¶è¡Œæ‰¹é‡ç¿»è¯‘
            stats = reverse_parallel_batch_translate(
                video_files,
                args.output_path,
                args.model_path,
                args.source_lang,
                args.target_lang,
                args.use_audio,
                args.save_audio,
                args.use_flash_attn,
                args.parallel_workers,
                args.shared_model,
                args.max_concurrent_gpu
            )

if __name__ == "__main__":
    main() 